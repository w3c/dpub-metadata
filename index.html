<!DOCTYPE html>
<html>
    <head>
        <title>
            Metadata Task Force Report
        </title>
        <meta charset='utf-8'>
        <script src='http://www.w3.org/Tools/respec/respec-w3c-common' async="" class='remove'></script>
        <script class='remove'>
        var respecConfig = {
          specStatus: "ED",
          shortName:  "dpub-metadata",
          editors: [
                {   
                  name:       "Bill Kasdorf",
                  url:        "",
                  company:    "BISG",
                  companyURL: "http://www.hbgusa.com",
                  mailto:     "bkasdorf@apexcovantage.com"  
                },
                { 
                  name:       "Madi Solomon", 
                  url:        "",
                  company:    "Pearson Plc.",
                  companyURL: "http://www.pearson.com",
                  mailto:     "madi.solomon@pearson.com" 
                },
                { 
                  name:       "Ivan Herman", 
                  url:        "http://www.w3.org/People/Ivan/",
                  company:    "W3C",
                  companyURL: "http://www.w3.org",
                  mailto:     "ivan@w3.org" 
                }
          ],
          previousMaturity:     "",
          previousPublishDate:  "",
          previousURI:          "",
          edDraftURI:           "http://w3c.github.io/dpub-metadata/",
          wg:                   "Digital Publishing Interest Group",
          wgURI:                "http://www.w3.org/dpub/IG/",
          wgPublicList:         "public-digipub",
          wgPatentURI:          "",
          noRecTrack:           true,
          otherLinks: [
            {
                key: "Repository",
                data: [{
                    value: "Github Repository",
                    href: "https://github.com/w3c/dpub-metadata"
                }]
            }
          ],
          alternateFormats: [
            {
                uri: "dpub-metadata.epub",
                label: "ePub"
            }
          ],
          charterDisclosureURI : "http://www.w3.org/2004/01/pp-impl/64149/status",
          copyrightStart : 2014
        };
        </script>
        <style type="text/css">
            a.footnote_sign {
                font-size: 70%;
                vertical-align: super;
                text-decoration: none !important;
            }
            /* Table zebra style... */
            table.zebra {
                font-size:inherit;
                font:100%;
                margin:1em;
            }
            table.zebra td {  
              padding-left: 0.3em;
            }

            table.zebra th {  
                font-weight: bold;  
                text-align: center;  
                background-color: NavyBlue !important; 
                font-size: 110%;
                background: hsl(180, 30%, 50%); 
                color: #fff;
            }

            table.zebra th a:link {
                color: #fff;
            }

            table.zebra th a:visited {
                color: #aaa;
            }

            table.zebra tr:nth-child(even) { 
                background-color: hsl(180, 30%, 93%) 
            }

            table.zebra th{border-bottom:1px solid #bbb;padding:.2em 1em;}
            table.zebra td{border-bottom:1px solid #ddd;padding:.2em 1em;}

            /* for commenting */
              aside {  
                width: 15em;  
                float: right;  
                color: black;  
                margin-top: 0.5em;  
                margin-right: 0.5em;  
                margin-bottom: 0.5em;  
                margin-left: 0.5em;  
                padding-top: 0.5em;  
                padding-right: 0.5em;  
                padding-bottom: 0.5em;  
                padding-left: 0.5em;  
                font-size: 0.9em;
                background: Lavender;
                border-left-width: .5em;
                border-left-style: solid;
                border-left-color: Orchid;
              }

              aside > p:first-child:before {  
                content: "COMMENTS: ";
              }

              .to_remove {
                text-decoration: line-through;
                background: yellow;
              }
              .to_add {
                background: hsl(129, 100%, 74%);;
              }
              li { margin-bottom: 0.5em;}
        </style>
    </head>
    <body>
        <section id='abstract'>
            <p>
                T.B.D.
            </p>
        </section>
        <section id='sotd'>
            <p>
                <em>This is a work in progress. No section should be considered final, and the absence of any content does not imply that such content is out of scope, or may not appear in the future. If you feel something should be covered here, tell us!</em>
            </p>
        </section>
        <section class='informative'>
            <h2>
                Overview
            </h2>
            <p>
                Publishers use metadata in three fundamentally different ways:
            </p>
            <ul>
                <li>
                   Metadata that is incorporated within a publication (e.g., an EPUB, a website).
                </li>
                <li>
                   Metadata that is separate from the publication or publications it describes (e.g., the periodic "metadata feeds" that publishers provide to the supply chain).
                </li>
                <li>
                   Metadata that is incorporated in systems designed to provide information about publications (e.g., a publisher’s, retailer’s, or aggregator’s website).
                </li>
            </ul>
            <p>
                While in many cases metadata is in a system or a form outside of the Web and using technologies outside of the Open Web Platform (OWP), such as databases, repositories, authoring and formatting software, and proprietary aggregation and dissemination platforms, OWP technologies are increasingly becoming essential to all aspects of the publishing process (including modern versions of all those mentioned). 
            </p>
            <p>
                The Metadata Task Force of the W3C Digital Publishing Interest Group (DPIG) was formed to identify ways in which the W3C could help address problems publishers currently have with regard to metadata. In its discovery phase, the TF found the following fundamental issues to be commonly regarded as "pain points" by publishers: 
            </p>
            <ul>
                <li>
                    <em>Granularity:</em> The need to associate metadata with arbitrarily granular units of content, rather than simply at the publication level.
                </li>
                <li>
                    <em>Complexity:</em> The profusion of identifiers and metadata vocabularies is confusing and difficult to master.
                </li>
                <li>
                    <em>Difficulty:</em> There are few systems or tools that enable metadata to be provided at appropriate stages of the workflow by non-technical people.
                </li>
                <li>
                    <em>Futility:</em> Even when rich metadata is provided, it often isn't used by the systems disseminating or displaying the publications.
                </li>
            </ul>          
            <p>
                Although each sector of publishing has problems with metadata in its own ways, the causes of these problems fall into two major categories:
            </p>
            <ul>
                <li>
                    Problems addressable by refinements to the Open Web Platform.
                </li>
                <li>
                    Lack of implementation, and even understanding, of existing features of the OWP on the part of publishers and others in the publishing supply chain.
                </li>
            </ul>
            <p>
                In its initial exploration of these issues, the Metadata Task Force of the W3C Digital Publishing Interest Group found that the vast majority of difficulties publishers of all types have in implementing metadata more effectively are in the second category. In most cases, the OWP already has features that address these issues, if used properly by publishers and implemented properly in systems that create, disseminate, and display those publications (e.g., expressing identifiers as URIs, using RDF and RDFa, etc.). In other cases, ongoing work by the W3C will likely provide solutions or essential components to solutions (e.g., the work of the new Open Annotations WG is closely related to the need to address arbitrarily granular units of content).
            </p>          
            <p>
                The Metadata Task Force of the W3C Digital Publishing Interest Group makes the following general and specific recommendations to the W3C with regard to the use of metadata within the OWP.
            </p>
            <ul>
                <li>
                    <em>Enable the use of existing vocabularies rather than creating new ones.</em> The vocabularies needed by different types of publishers vary tremendously, and in order to be effective they tend to be highly specific to certain disciplines, use cases, or sectors. A great number of these vocabularies already exist and are in wide use within the communities they were designed to serve. The role of the W3C should be to provide means to enable publishers and others in the publishing ecosystem to efficiently and effectively use those vocabularies in the three modes described at the beginning of this section. <em>Specific recommendation:</em> The W3C should collaborate with BISG, the Book Industry Study Group; EDItEUR, the international organization responsible for ONIX, the standard messaging format for dissemimation of book supply chain metadata; and schema.org, which provides the most commonly used means of embedding metadata in web content; and other appropriate parties to develop an optimal way for book publishers to embed appropriate and useful metadata in Web content based on the well known and widely implemented ONIX model. The BISG has already formed a Working Group to address this issue from the point of view of its US constituency, and that WG is chaired by the Executive Director of EDItEUR, who can ensure that the resulting recommendations are globally applicable.
                </li>
                <li>
                    <em>Educate publishers and their partners on how best to use existing features of the OWP.</em> While technical specifications, sometimes supplemented by primers, are already provided on such features by the W3C, these are often targeted at technical users. There is a need for much simpler, more user-friendly documentation aimed at non-technical people within the publishing ecosystem. There is also a need for much more aggressive dissemination of this information throughout the publishing ecosystem to demystify these features of the OWP and encourage their broad and proper use both by creators and recipients of metadata. <em>Specific recommendation:</em> Focus initially on encouraging proper understanding and use of the URI and of RDF/RDFa. As an example, CrossRef--which provides (among other services) cross-publisher linking from citations to cited publications using the DOI (the identifier that has become fundamental to the scholarly publishing ecosystem)--has for some time recommended that DOIs always be expressed in the form of a URI. However, this is still seldom done because of the lack of understanding of the URI by publishers and other participants in the publishing ecosystem. Likewise, ISBNs--the primary product identifier for books--can also be expressed as a URI. Better understanding of the URI and how to use it would be an important step in the improvement of metadata implementation within Web content and systems. Likewise, many publishers who lament the lack of ability to describe their content at a very granular level are unaware of, or have only a vague understanding of, RDF and RDFa, and these technologies are seldom implemented in the systems used for the discovery and dissemination of publications. Better understanding of RDF and RDFa across the publishing ecosystem could promote the use of a technology that is currently significantly underutilized. [TF: We no doubt need to make a much more specific recommendation here, but this is a start.]
                </li>
                <li>
                    <strong>[More recommendations to follow from the TF. E.g., I think we should definitely make some sort of recommendation with regards to rights metadata. To be discussed.]</strong>
                </li>
            </ul>
        </section>
        <section class='informative'>
            <h2>
                Publishing Terms and Concepts
            </h2>
            <p>
                <em>[This section should provide very brief definitions of the key terms that need to be understood before reading the balance of the document and which may not be understood by readers within the W3C (e.g., ONIX, DOI, CrossRef, BISAC, Thema etc.). They should link to a Glossary which contains a more comprehensive set of terms used in this document (which should be at the end--it would be distracting here), each of which should link, as appropriate, to authoritative resources. Thus ONIX might be briefly defined as I described it above; the glossary would provide a 2-3 sentence description and would link out to the EDItEUR's ONIX page. Ditto for DOI, CrossRef, BISAC, Thema, etc.]</em>
            </p>  
        </section>
        <section class='informative'>
            <h2>
                Background
            </h2>
            <p>
                In order to assess the “pain points” with regard to metadata for publishers, the co-leaders of the task force, Madi Solomon of Pearson and Bill Kasdorf of Apex, conducted a number of interviews with publishers, service providers, and representatives from related organizations. The inverviews themeselves are available <a href="#interviews">in the appendix</a> of this document.
            </p>
            <p>
                The interviewees were selected to provide insight from a variety of perspectives, and were individuals known to the interviewers to be knowledgeable and authoritative within their spheres. Ms. Solomon took a “vertical” approach, interviewing a broad range of individuals within Pearson, a large global educational publisher. Mr. Kasdorf took a “horizontal” approach, interviewing experts from diverse types of publishing (book, journal, magazine, and news) and representing diverse roles within the digital publishing ecosystem (publishers, metadata service providers, consultants, and representatives from other organizations that are addressing the issue of metadata in publishing).
            </p>
            <p>
                The interview strategy was to conduct casual, open-ended interviews with a single individual<a class="footnote_sign" href="#_ftn1" title="" id="_ftnref1">1</a> without an agenda or a prepared set of questions. The reason for this strategy was to avoid steering the discussion in particular directions. Instead, in this initial phase, the goal was to elicit what each interviewer would perceive as the key issues and pain points with regard to metadata from their own point of view. Thus the interviews deliberately did not focus on the issue of what the W3C could do-and what changes could be made to the Open Web Platform to address them. Instead, the interviews stayed on the general level. Since many of the interviewees were not technical, framing the discussion in too technical a manner would have impeded the ability to obtain authentic responses. As expected, few of the interviewees felt able to identify specific “pain points” with regard to the OWP. They spoke instead of general issues of concern to them in their work. The hope was that with an understanding of these issues and pain points, the DPIG could then assess where the W3C and the OWP could potentially help address them-and could avoid addressing theoretical technical issues that might not in fact align with publishers’ priorities.
            </p>
            <p>
                While the published interview reports cited above will provide the best understanding of both the common themes and diverse perspectives revealed by the interviews, this report attempts to summarize key observations and offer initial recommendations for subsequent strategies.
            </p>
        </section>
        <section class='informative'>
            <h2>
                Primary Observations
            </h2>
            <p>
                If there is a single overarching lesson revealed by these interviews, it is that the issues with regard to metadata for publishers and their clients and partners differ significantly between publishing sectors.
            </p>
            <ul>
                <li>
                    <em>Trade book publishers are primarily concerned with discovery:</em> providing metadata to the supply chain that will attract readers to their books and result in increased sales. It was observed that at the present time book content itself is not typically online; it is available digitally only as products (e.g., EPUB), and typically only at the title (book) level, and the metadata regarding those products (primarily in <a>ONIX</a>) is created, maintained, and disseminated independently of the products themselves.
                </li>
                <li>
                    <em>Educational publishers see metadata primarily in the context of asset and content management:</em> the identification and characterization of granular components of content within a large repository of content in order to facilitate the creation of publications, the delivery of content in “chunks,” the repurposing of content to create new editions and new products, and the ability to guide students and teachers to highly targeted components of content. The ability to personalize content to individual students, the ability to associate learning objectives with arbitrarily granular components of content, and the ability to monitor and assess the use of content by students and the learning outcomes that result are all key metadata priorities for educational publishers.
                </li>
                <li>
                    <em>Scholarly publishers see metadata mainly as a “solved problem”:</em> while none would assert that the current situation is perfect, the standards-based consensus in the scholarly publishing world-consisting of nearly universal participation in <a>CrossRef</a> and CCC (the Copyright Clearance Center), the ubiquitous use of the <a>JATS</a> XML model<a class="footnote_sign" href="#_ftn2" title="" id="_ftnref2">2</a> for markup and metadata, and the reliance upon the <a>DOI</a> as a persistent, actionable identifier—initially for journal articles but now increasingly for book chapters and components, reference content, conference proceedings, and other publications, as well as the data sets that support research)—has enabled the development a rich ecosystem of services and platforms that has made the Web the primary mode of publication, dissemination, and access for scholarly content. It has also led to the development of other standards-such as <a>ORCID</a>, the Open Researcher and Contributor ID, and <a>FundRef</a>, a system for making public the funders of research-that continually refine the sophistication and utility of metadata in the scholarly publishing world, solving what were previously significant pain points (e.g., disambiguating contributor identities, revealing potential conflicts of interest or reliably documenting the absence of such conflicts).
                </li>
                <li>
                    <em>Magazine publishers and news organizations are currently focused on rights metadata:</em> the rapid shift to the Web as a source of news, entertainment, and information has created an urgent need to identify the owners of content and the rights associated with very granular components of content-both the rights that apply broadly to a particular unit of content (e.g., copyright) and the usage rights conveyed by the rightsholder to specific parties, in specific contexts, for specific purposes, and in specific modes. While there are well established metadata models in each of these areas-<a>PRISM</a> is a very rich metadata framework virtually universal in the magazine world, and standards such as <a>IPTC</a> photo and media metadata, <a>rNews</a>, <a>newsML</a>, and <a>schema.org</a> are widely used by news organizations-standards bodies in both of these areas (IDEAlliance for magazines and <a>IPTC</a> for news) are each actively working on developing and refining rights metadata models.
                </li>
            </ul>
            <p>
                While all of these issues-discovery via subject metadata and other metadata characterizing content and products, management of content via metadata, development and participation of cross-publisher platforms and services via metadata, and the communication of rights via metadata-cross all sectors of publishing, it is clear from the interviews that the priorities in distinct sectors diverge significantly.
            </p>
            <p>
                Another major theme heard in virtually all of the interviews was that metadata is “too complicated.” Book publishers, for example, recognize that <a>ONIX</a> is the standard way to communicate supply chain metadata; as such, it is an extremely rich, complex, and useful standard. Similarly, the <a>BISAC</a> standard is a rich vocabulary used in the US for subject classification; there are similar such standards in most other countries or regions, and also a new global standard, Thema. While publishers recognize the value of these standards, they often characterize them as “too hard”; yet when pressed for what an individual publisher needs to communicate (to the supply chain, or about the subjects of its books), they often wind up asking for more complexity. (E.g., a U.S. publisher may want to describe a book as being about “the Battle of the Bulge, within the topic “World War I” which itself is in the category of military history; this can be done with <a>BISAC</a> but not with <a>Thema</a>.) The truth is that these systems are complex because what they are designed to do is complex. The desire for an “ONIX Lite” expressed by several interviewees may prove to be unrealistic, because a significantly simpler model would be significantly less expressive.
            </p>
            <p>
                Another common theme was that in too many cases metadata may exist-or may potentially exist, if applied to a given publication-but it often “doesn’t do anything.” It is very frustrating to users if it is the case-or even if it is their perception-that going the work to adding metadata is futile because systems are not seen as using it. (This is of course true of some types of metadata but not others: clearly trade publishers know how their <a>ONIX</a> metadata is used by the supply chain, and scholarly publishers know how their <a>CrossRef</a> metadata is used for citation linking.) This particularly surfaced in the context of the Pearson interviews because complex educational content is created by a vast team of participants, each of whom may have the ability to provide some aspect of metadata but most of whom have no clear understanding of how to do so, no systems to enable to do that consistently, and no faith that if they “go to all that work,” it will actually be used for any purpose downstream.
            </p>
            <p>
                In thinking about metadata, it is important to distinguish between metadata that is incorporated within a publication (an EPUB, a website); metadata that is separate from the publication or publications it describes (e.g., <a>ONIX</a>, which can continually change over time without requiring the publications it communicates metadata about to be altered); and metadata that is incorporated in systems designed to provide information about publications (e.g., a publisher’s, retailer’s, or aggregator’s website).
            </p>
            <p>
                And finally, it should be noted that an important theme that <em>did not</em> emerge from the interviews was the importance of accessibility. Revealing this was one of the benefits of the interview strategy of not asking leading questions: when anybody is asked if accessibility is an important issue, they will almost always say it is. So it is particularly—and lamentably—of note that none of the interviewees mentioned accessibility as a priority issue with regard to metadata.
            </p>
        </section>
        <section class="informative">
            <h2>
                Important Themes
            </h2>
            <p>
                The key themes of the interviews conducted by Mr. Kasdorf are summarized in the following <span>appendix</span>. They are the following:
            </p>
            <ul>
                <li>Complexity</li>
                <li>Inconsistency</li>
                <li>Sacrificing Richness for Simplicity</li>
                <li>ONIX vs. Subject Metadata</li>
                <li>Few Books are Online Anyway</li>
                <li>Discovery (AKA Marketing) is the Priority</li>
                <li>Identifiers, Identifiers, Identifiers</li>
                <li>And Now for Something Completely Different: News</li>
            </ul>
            <p>
                Please see the <a href="https://www.w3.org/dpub/IG/wiki/Task_Forces/Metadata#Phase_1_Strategy">summary in the appendix</a> for a discussion of these themes, including important comments by members of the DPIG.
            </p>
            <p>
                The key themes of the interviews conducted by Ms. Solomon are summarized in the report provided at [TK]. They are the following:
            </p>
            <ul>
                <li>Governance</li>
                <li>Rights</li>
                <li>Flow</li>
                <li>Lack of Skills</li>
                <li>Lack of Authority</li>
                <li>Standards</li>
                <li>Inconsistency</li>
                <li>Lack of Incentives</li>
                <li>Need for Learning Objectives</li>
                <li>Need for a Centralised Authority or Entity</li>
                <li>More Education and Guidance</li>
            </ul>
            <p>
                Please see <a href="#vertical">Ms. Solomon’s report below</a> for a more detailed discussion of these themes.
            </p>
        </section>
        <section class="informative">
            <h2>
                Initial Recommendations
            </h2>
            <p>
                On the basis of these interviews and the themes and priorities that they revealed, the DPIG suggests the following with regard to how the W3C could/should or could not/should not attempt to address these issues in the context of the Open Web Platform. These should be considered in the context of the distinction between metadata embedded in content, metadata embedded in web pages about the content, and metadata that is entirely separated from the content. [To be augmented and refined by the DPIG]
            </p>
            <aside>
              <p>Notes from ivan...</p>
              <p>On #1: the relationship to schema.org should be clarified. The text here suggests that <em>nothing</em> should be done, although the subsequent discussions suggested that work should be done to map (part of) ONIX to schema, without the pretention of defining a complete vocabulary. This should be formulated here. Also, emphasizing that anything done here should be done in collaboration with, eg, BISG as much as possible may be important.</p>
              <p>On #2: I must admit I do not understand what is meant here</p>
              <p>On #3: I must admit I do not understand what is meant here. Are we discussing issues around anchoring and it granularity, the usability of CFI?</p>
              <p>On #4: is the suggestion here that W3C should define a rights expression language? This issue came up elsewhere at W3C, so this advise may be very timely. Question is whether there is really a need to standardize this and whether W3C is indeed the right place to do so.</p>

            </aside>
            <ol>
                <li>
                    <em>Make no attempt to develop standard vocabularies.</em> There are a great many vocabularies (controlled vocabularies, taxonomies, and ontologies) that exist through the publishing realm and the constituencies (readers, scholars, booksellers, libraries, teachers, students, scientists, etc.) that publishers serve. Useful vocabularies almost always need to be discipline-specific and context-dependent. While schema.org (not a W3C standard, but one closely related to the Open Web Platform) provides a valuable high-level mechanism for standardizing certain properties and in some cases controlled vocabularies associated with those properties, the W3C should not attempt to go further.
                </li>
                <li>
                    <em>The focus of the W3C should be to enable publishers to incorporate and use the metadata that is specific to their publications and use cases.</em> While the markup and metadata models that are widely used in certain domains-e.g., <a>JATS</a> for scholarly and <a>STM</a> publishing, <a href="docbook.org">DocBook</a> for technical publishing, and TEI for scholarship and humanities research-all provide extensive “metadata headers” as fundamental components of their schemas, no such metadata component explicitly exists within the OWP (although the OWP-through HTML5, microdata, and <a>RDFa</a>-does provide generic mechanisms for managing and conveying such metadata). The result of such an activity might result in recommendations and best practices by which publishers can use the existing components of the OWP to manage and convey metadata rather than the creation of new features of the OWP.
                </li>
                <li>
                    <em>The need to associate subject metadata and other metadata characterizing content with arbitrarily granular units of content is a priority.</em> While, as mentioned above, the ability to do this may already be sufficiently provided within the OWP, the lack of implementations retards its use in this way by publishers. It is ironic that something that is seen as a quite urgent need by publishers does not seem to be seen as an urgent priority by the developers of reading systems and tools.
                </li>
                <li>
                    <em>The ability to associate rights metadata with arbitrarily granular units of content is a priority.</em> This goes far beyond the typical binary view of rights. It involves the ability to provide interoperable information about permissions and prohibitions associated with a unit of content by certain parties in a given context; the ability to convey or provide access to contractual information associated with the use of content components between the rightsholder and the end user; the ability to dynamically modify the rights information over time and in a context-dependent manner; and ideally the ability to monitor compliance with or violations of the rights associated with content.
                </li>
            </ol>
        </section>
        <section class="appendix">
            <h2 id="interviews">
                List of interviews
            </h2>
            <section>
                <h3>
                    “Horizontal” Interviews (Notes by Bill Kasdorf)
                </h3>
                <section>
                    <h2 id="laura-dawson">
                        Laura Dawson, Bowker
                    </h2>
                    <p>
                        Laura’s first observation was that there is “so much metadata available—that’s not the problem.” She said that “wrangling all the descriptions” is really the problem, because of the proliferation of vocabularies and terms.
                    </p>
                    <p>
                        The biggest pain point for publishers in her view is “<strong>the problem of updates.</strong>” Hachette sends metadata to X recipient; that metadata doesn’t see the light of day for three weeks; in the meantime the metadata has been updated in a weekly feed. Feeds can overwrite other feeds.
                    </p>
                    <p>
                        Another key problem: different versions needed for different recipients: “there is no one true <a>ONIX</a>.”
                    </p>
                    <p>
                        Issues are not as much about the structure, or about the vocabularies, but how they are used.
                    </p>
                    <p>
                        There is also a vocabulary issue re <strong>subjects of components</strong>: <a>BISAC</a> doesn’t work, “not granular enough.” “If you really want to embed metadata in a meaningful way, you can’t do it.” (She mentioned <A>IPTC</A> codes in this context.)
                    </p>
                    <p>
                        She is a big fan of <a href="http://schema.org">schema.org</a>. Publishers need to use identifiers for people (<a>ISNI</a>, <a>ORCID</a>).
                    </p>
                    <p>
                        What’s needed is some combination of bibliographic ontology and relations.
                    </p>
                    <aside>The Wiki page referred to "BibEx" group, but I suppose that was a mistake, changed it to the bib extend Community group…</aside>
                    <p>
                        May need different types for selling books vs. finding them. Library schemes (e.g., <a href="http://www.w3.org/community/schemabibex/">Bib Extend Community Group</a>) primarily focus on libraries, which have properties like “holdings” and “checking out a book” that are irrelevant to publishers.
                    </p>
                    <p>
                        She said that “Google should be the primary audience—that’s how people find books.” <strong>Search engines</strong> and the indexing they do are key. Perhaps making this information <strong>viewable</strong> is the key, the link back to the OWP and W3C—“even if the content isn’t available on the open web.”
                    </p>
                </section>
                <section>
                    <h2 id="fran-toolan">
                        Fran Toolan, CEO, Firebrand
                    </h2>
                    <p>
                        Fran characterized the biggest problem as “Publishers don’t understand the role of the web. They understand that everyone is on it, but they are confused by all the virtual storefronts, and very confused about SEO, how to determine keywords, how does book industry metadata (<a>BISAC</a>) fit in. They don’t have workflows around this. They don’t have somebody on staff looking at Google AdWords for every title.”
                    </p>
                    <p>
                        Another big problem: every retailer site has its own ingestion engine, its own specifications.
                    </p>
                    <p>
                        The metadata itself isn’t as bad as people think it is. The vast majority of publishers are creating the metadata they’ve been told to create.
                    </p>
                    <p>
                        Another key insight from Fran: “Metadata was once identified as a method by which a publisher could control the perception of the book. That is no longer the case. With all the review sites, social media, etc., it is now out of the publisher’s control. The publishers don’t know what to do about this.”
                    </p>
                    <p>
                        “The right question is ’How can publishers be more successful on the Web?’”
                    </p>
                    <p>
                        He feels that metadata in the context of the W3C “means something completely different than what it means in the book supply chain.” “What’s ’the product’ on the Web?”
                    </p>
                    <p>
                        There’s confusion regarding usage. Most people consider usage info as “metadata”: how much of the book can display, where it can be sold, who has the rights to it, etc. This is really supply chain data.
                    </p>
                    <p>
                        <strong>“<a>ONIX</a> is irrelevant to the W3C.“</strong> It doesn’t show up anywhere on the Web.
                    </p>
                    <p>
                        “What the publisher cares about is if the book can be discovered.” The Web page should be an aid to discovery. <strong>“But the publisher is not in control of that because they’re not the retailer.”</strong>
                    </p>
                    <p>
                        Biggest problem: the publisher doesn’t know what affects what. “Is the info they’ve been browbeaten to give over the past few years doing any good? No way to know, no feedback loop.”
                    </p>
                    <p>
                        He feels that “<a>BISAC</a> categories are irrelevant to the W3C.” How can this translate to the W3C? <strong>Keywords are a big deal.</strong> <em>(Note that BISG is working on a keywords recommendation—not a vocabulary, but guidelines regarding keywords.)</em>
                    </p>
                    <p>
                        “<a>CrossRef</a> and <a href="http://schema.org">schema.org</a> work because they’re each a central registration agency.” “Bowker and Books In Print used to be that in the print era. Currently there is no single repository for book metadata.” “The only way to get a central registration agency for book metadata is if the retailers agree to use it. Amazon and Apple will never do that.” “Retailers like Walmart etc. use <a>UPC</a> (bar codes) for which <a>GS1</a> is the central registration agency <em>(for general merchandise)</em>. This never got any legs for books.”
                    </p>
                </section>
                <section>
                    <h2 id="thad-mcilroy">
                        Thad McIlroy, Consultant and co-author, The Metadata Handbook
                    </h2>
                    <p>
                        “<a>ONIX</a> and metadata in general are just way too complex for the average trade publisher.” “These people may never come along.”
                    </p>
                    <p>
                        “We need ONIX LITE focused on marketing and discovery.”
                    </p>
                    <p>
                        “Very few publishers have the resources of a Pearson to get <a>ONIX</a> right.”
                    </p>
                    <p>
                        “The metadata situation for a receiver is just crazy—TONS of inconsistency.”
                    </p>
                    <p>
                        “The use case is not clear—what is the benefit?”
                    </p>
                    <p>
                        “There’s also lots of inconsistency in what the recipients require.”
                    </p>
                    <p>
                        The <a>ONIX</a> 3.0 vs. 2.1 situation is a real problem.
                    </p>
                    <p>
                        “Today, in 2014, the point of metadata is marketing books and making them discoverable. We should just focus on that.”
                    </p>
                    <p>
                        “Maybe <a>schema.org</a> should become ONIX LITE.”
                    </p>
                </section>
                <section>
                    <h2 id="renee-register">
                        Renee Register, Consultant and co-author, The Metadata Handbook
                    </h2>
                    <p>
                        “Getting people to use the same model and metadata is the big issue. Everybody—Amazon, Books In Print, B&amp;N, Baker &amp; Taylor, etc. etc.—all use proprietary models. There are huge proprietary databases. There is tremendous duplication of effort.”
                    </p>
                    <p>
                        “Publishers feel as if they don’t have control of the data.”
                    </p>
                    <p>
                        “There’s a big need for metadata remediation, cleanup.”
                    </p>
                    <p>
                        “It looks different on Ingram, Amazon, B&amp;N, B&amp;T, etc.—it’s all over the place.”
                    </p>
                    <p>
                        She “would love to see a central repository.”
                    </p>
                    <p>
                        “Inputting <a>ONIX</a> is too difficult—there are no good tools or mechanisms to create <a>ONIX</a>.”
                    </p>
                </section>
                <section>
                    <h2 id="vincent-baby">
                        Vincent Baby, Chairman of the Board, International Press Telecommunications Council (<A>IPTC</A>)
                    </h2>
                    <p>
                        Vincent Baby, Chairman of the Board of the <a href="http://www.iptc.org">IPTC</a> (International Press Telecommunications Council), the major source of metadata standards for the news industry, including the widely used <A>IPTC</A> Photo Metadata schemas, the XML-based <a>NewsML</a>, the <a>RDFa</a>-based <a>rNews</a>, <a>RightsML</a>, and others. </p>

                    <p class="note">
                        Mr. Baby—a journalist by training, currently in a product management role with Thomson Reuters—made it clear that his comments in our conversation were his own personal views; he was in no way speaking for the <A>IPTC</A> or Thomson Reuters.
                    </p>
                    <p>
                        Here are some links to relevant <A>IPTC</A> resources sent by Mr. Baby: “<A>IPTC</A> is constantly updating core standards like <a href="http://www.iptc.org/site/Photo_Metadata/">photo metadata</a> and <a href="http://www.iptc.org/site/News_Exchange_Formats/NewsML-G2/">NewsML</a> and crafting new standards for <a href="http://dev.iptc.org/rNews">semantic annotation of news items</a> on the Web, <a href="http://dev.iptc.org/RightsML">embeddable rights expressions</a> and <a href="http://dev.iptc.org/ninjs">APIs</a>. We have also recently revived our <a href="http://www.iptc.org/site/NewsCodes/">NewsCodes</a> Working Party which develops and maintains provider-agnostic taxonomies.”
                    </p>
                    <p>
                        He began by pointing out that the <A>IPTC</A> and W3C overlap and intersect in many ways; currently, the main link is <a>ODRL</a>, the Open Digital Rights Language. <A>IPTC</A> has done a lot of work on a rights expression language that he thinks could be very valuable to publishers of all sorts (most others are unaware of it). Recognizing that the “free text” most such models enable is ineffective, <A>IPTC</A> focused on developing machine-processable metadata. Their <a>RightsML</a> is <a>ODRL</a> with a vocabulary specific to the news industry.
                    </p>
                    <p>
                        The <a href="http://www.w3.org/community/semnews/">Semantic News Community Group</a> at the W3C “emanated from <A>IPTC</A> members.” However, this activity has been pretty dormant for the past 2-3 years. He commented that a number of <A>IPTC</A> members are “wary of the W3C” because of IP concerns. However, he mentioned that one of their constituent organizations, <a href="http://www3.ebu.ch/home">EBU</a> (the European Broadcasting Union in Geneva) has been an active participant in the W3C.
                    </p>
                    <div class="note">
                    <p>
                        The story of rNews is interesting in the context of the other conversations I’ve been having with people from other sides of publishing. The New York Times brought the use case to the <A>IPTC</A> initially: the need to preserve metadata in HTML pages. News publishers have very rich metadata in their CMSs but it is lost in the content that goes online. What was developed was <a>rNews</a>, an RDFa-based model that makes news more accessible to search results and to social media and allows better displays such as “rich snippets”.
                    </p>
                    <p>
                        Here’s the part of the story I found most resonant: in June 2011 they became aware of schema.org, and there was a concern that schema.org would overshadow <a>rNews</a>. They contacted schema.org and found that schema.org was in fact very interested in collaborating and open to the input of representative organizations such as <A>IPTC</A> in various domains. The result was that schema.org now incorporates a small subset of <a>rNews</a> properties. This resonated with comments others have made about the need for an “ONIX Lite” or some similar ability to get subject metadata into schema.org. BTW. in my view it is probably <a>Thema</a>, the new international subject codes standard, not <a>ONIX</a>, that’s the best candidate for this.
                    </p>
                  </div>
                    <p>
                        The result is that many <a>rNews</a> properties are now widely adopted in the news industry, though mainly the subset that’s in <a>schema.org</a>. The big players like the BBC use the full <a>rNews</a> schema.
                    </p>
                    <p>
                        In discussing the challengers publishers face, Mr. Baby named two fundamental issues:
                    </p>
                    <ol>
                        <li>Maximizing engagement with the user base
                        </li>
                        <li>The need for efficiency and economy
                        </li>
                    </ol>
                    <p>
                        He pointed out that metadata can have a role in both areas.
                    </p>
                    <p>
                        He remarked that publishers initially focused on SEO; but that proved ineffective because just getting a user to a given web page didn’t build any lasting value or connection with that user, or knowledge about that user. Now they are more focused on social media and semantic markup.
                    </p>
                    <p>
                        An important aspect of the ecosystem today is the importance of rich multimedia content. Cross-media alignment between assets has become critically important. The problem is that there are different taxonomies and different levels of richness associated with different types of media assets. For example, video has great technical metadata but very poor subject metadata.
                    </p>
                    <p>
                        It’s not just about text anymore. It’s about text&nbsp;+&nbsp;pictures, interactivity, databases, Twitter, and on and on. This multiplicity of media formats presents a huge challenge: how do you keep track of everything?
                    </p>
                    <p>
                        On the subject of engagement, one thing he lamented was that embedded metadata winds up getting stripped out when it gets to Twitter, Facebook, etc.: most social networks are “cavalier about metadata—they just toss it out.” He mentioned an “Embedded Metadata Manifesto” that is hoping to counter this [Link provided below].
                    </p>
                    <p>
                        Another important issue: how do you measure impact? He said that “all kinds of initiatives are working on this.” Over time, publishers will want to add this information to their metadata, e.g. “this is a bestseller” or “this has been retweeted X times.” [Note: that is already intrinsic to ONIX for the book supply chain.]
                    </p>
                    <p>
                        He had a lot of interesting and resonant points to make in the context of efficiency. [I think a lot of what he pointed out regarding news will become increasingly relevant to most types of publishing, which are moving from a “once-and-done” model to a situation where content evolves over time.]
                    </p>
                    <p>
                        An area I found particularly relevant to work in other areas was his discussion of “how to automate workflows with metadata?” and “how to mark up content in a way that makes it easy to reuse chunks?” In the news industry, it is common for new versions of a story to build on previous versions, with lots of overlap. But often, when there is only a small proportion of updated content, it becomes a whole new story.
                    </p>
                    <p>
                        One aspect of this is the need for granular structural semantic markup: What is a quote? What is a “background paragraph”? What is a “lede”? etc. There is a need to “isolate these bits” so that the user can be presented with “just the new bits.”
                    </p>
                    <p>
                        Many people are putting their hopes in an “event-based” model where ”unique events” can be identified in advance, with an ID, and then managed over time. Others, including for example the BBC and the Guardian, object that this does not work because this is not how journalists actually work. New stories pop up unexpectedly and then twig off in unexpected directions that can’t be anticipated in advance. (E.g., Ukraine rejects the deal with the EU and a few months later Crimea has been annexed to Russia, with lots happening in between that nobody would have expected.) Newsrooms typically identify the successive iterations of stories using “slugs” (keywords) that enable users (and journalists) to follow how a story evolves. The “storyline” model aims to organize these free-form slugs ex-post using a dedicated ontology, thereby leveraging in the background a pre-existing workflow.
                    </p>
                    <p>
                        In this context, archives have become increasingly important: they help get more engagement from readers. This is another issue that crosses media types. He mentioned that just this week, ProPublica has posted a very interesting draft on their work on how to archive “news applications.” This is based on the need for an interactive database that is queryable on a very local level. How do you archive the stories that make up that repository to enable this to work? (<a href="#iptc-links">See below</a> for further links.)
                    </p>
                    <p>
                        Another key issue in all this is the human dimension. He pointed out that “journalists are very creative people,” interested in the <em>substance</em>, not the process. They’re generally not disciplined about cataloging, labeling, etc. So “metadata becomes a struggle.” Management, on the other hand, recognizes that this metadata is crucial: it’s what makes the content useful to both users and publishers.
                    </p>
                    <p>
                        Having said that, there can be a “blind faith” that putting the systems and processes in place will resolve the problems: “If we fix our metadata everything will be hunky dory.” This is obviously naïve.
                    </p>
                    <p>
                        Finally, he raised the familiar issue of the proliferation of devices, OS’s, form factors, etc. He pointed out that during a recent weekend 55% of The Guardian’s content was viewed via mobile, where there are hundreds of different devices and form factors. Therefore, responsive design becomes crucial. A challenge: everything needs to be tested for compatibility across a broad range of browsers and devices.
                    </p>
                    <p id="iptc-links">
                        He closed our conversation with an offer to provide links to a number of useful resources—which in fact he did about an hour after our conversation. Here’s what he sent:
                    </p>

                    <ul>
                    <li>
                        <a href="http://www.w3.org/community/semnews/"><code>W3C Semantic News Community Group</code></a> (purely for reference)
                    </li>
                    <li>
                        Leading edge innovators in the news publishing industry
                        <ul>
                            <li>FT Labs: <a href="http://labs.ft.com/"><code>http://labs.ft.com/</code></a>
                            </li>
                            <li>NYT Labs: <a href="http://nytlabs.com/"><code>http://nytlabs.com/</code></a>
                            </li>
                            <li>Knight Mozilla OpenNews: <a href="http://opennews.org/"><code>http://opennews.org/</code></a>
                            </li>
                            <li>BBC <a href="http://www.bbc.co.uk/blogs/internet/"><code>http://www.bbc.co.uk/blogs/internet/</code></a>
                            </li>
                        </ul>
                    </li>
                    <li>
                      Structured news
                      <ul>
                          <li>Circa <a href="http://www.fastcolabs.com/3008881/tracking/circas-object-oriented-approach-to-building-the-news"><code>http://www.fastcolabs.com/3008881/tracking/circas-object-oriented-approach-to-building-the-news</code></a>
                          </li>
                          <li>Storyline Ontology <a href="http://www.bbc.co.uk/ontologies/storyline/2013-05-01.html"><code>http://www.bbc.co.uk/ontologies/storyline/2013-05-01.html</code></a>
                          </li>
                          <li>Structured Journalism (Reg Chua) <a href="http://structureofnews.wordpress.com/structured-journalism/"><code>http://structureofnews.wordpress.com/structured-journalism/</code></a>
                          </li>
                      </ul>
                    </li>
                    <li>
                        Dealing with non standard media formats
                      <ul>
                          <li>ProPublica <a href="http://www.propublica.org/nerds/item/a-conceptual-model-for-interactive-databases-in-news"><code>http://www.propublica.org/nerds/item/a-conceptual-model-for-interactive-databases-in-news</code></a>
                          </li>
                          <li>CMSes <a href="http://www.fastcolabs.com/3022755/whats-so-hard-about-about-building-a-cms"><code>http://www.fastcolabs.com/3022755/whats-so-hard-about-about-building-a-cms</code></a>
                          </li>
                      </ul>
                    </li>
                    <li>
                        Semantic Web (for news)
                      <ul>
                          <li>rNews, schema.org and New York Times <a href="http://open.blogs.nytimes.com/2012/02/16/rnews-is-here-and-this-is-what-it-means/"><code>http://open.blogs.nytimes.com/2012/02/16/rnews-is-here-and-this-is-what-it-means/</code></a>
                          </li>
                          <li>rNews at the BBC <a href="http://www.bbc.co.uk/blogs/internet/posts/News-Linked-Data-Ontology"><code>http://www.bbc.co.uk/blogs/internet/posts/News-Linked-Data-Ontology</code></a>
                          </li>
                          <li>Dynamic Semantic Publishing at the BBC <a href="http://www.bbc.co.uk/blogs/bbcinternet/2012/04/sports_dynamic_semantic.html"><code>http://www.bbc.co.uk/blogs/bbcinternet/2012/04/sports_dynamic_semantic.html</code></a>
                          </li>
                      </ul>
                    </li>
                    <li>
                        Embedded metadata
                      <ul>
                          <li>Embedded Metadata Manifesto <a href="http://www.embeddedmetadata.org/"><code>http://www.embeddedmetadata.org/</code></a>
                          </li>
                      </ul>
                    </li>
                    <li>
                      Measuring impact
                      <ul>
                          <li>CIR <a href="http://www.niemanlab.org/2014/03/how-can-journalists-measure-the-impact-of-their-work-notes-toward-a-model-of-measurement/"><code>http://www.niemanlab.org/2014/03/how-can-journalists-measure-the-impact-of-their-work-notes-toward-a-model-of-measurement/</code></a>
                          </li>
                          <li>Nieman Lab <a href="http://www.niemanlab.org/2012/08/metrics-metrics-everywhere-how-do-we-measure-the-impact-of-journalism/"><code>http://www.niemanlab.org/2012/08/metrics-metrics-everywhere-how-do-we-measure-the-impact-of-journalism/</code></a>
                          </li>
                          <li>Media Impact Project <a href="http://www.learcenter.org/html/about/?cm=mediaimpact"><code>http://www.learcenter.org/html/about/?cm=mediaimpact</code></a>
                          </li>
                      </ul>
                    </li>
                </section>
                <section>
                    <h2 id="michael-steidl">
                        Michael Steidl, Managing Director, International Press Telecommunications Council (<A>IPTC</A>)
                    </h2>
                    <p>
                        My interview with Michael was an excellent complement to the interview I conducted with Vincent Baby, Chairman of the Board of the <A>IPTC</A>. Whereas Vincent is part of the volunteer leadership (he works for Thomson Reuters), Michael’s full time responsibility is the very wide-ranging work of the <a href="http://www.iptc.org">IPTC</a>, an organization that has been deeply involved in the development and use of metadata standards for 35 years. Its standards are extensively implemented globally, primarily in the context of news (including textual, image, and multimedia content).
                    </p>
                    <p>
                        Although his focus is of course not primarily on books, Michael began by observing that the concept of “book” is evolving due to the emergence of digital books, along with print: a “book” really becomes the intellectual content, not just a product, and even the nature of that intellectual content is changing.
                    </p>
                    <p>
                        Similarly, in the <A>IPTC</A>’s area, the question “what is news?” is evolving for many of the same reasons. It can mean “professional news,” or it can be broadened to include blogging, news created by private individuals, etc. <A>IPTC</A> focuses on the former: they represent “the professional creation and distribution of news,” not everything that might be new, or “every 10th tweet would be news.”
                    </p>
                    <p>
                        <A>IPTC</A> has a long history of metadata work. Their first standard with metadata was created in 1979, and it is still in use: “<a>IPTC 7901</a>,” which is a sibling to “ANPA 1312” in the US. (There are only minor differences—they are “95% in common.”)
                    </p>
                    <p>
                        <A>IPTC</A> also began to focus on multimedia early on, creating <a>IIM</a> (their Information Interchange Model) which was adopted by Adobe in 1994. This was the origin of photo metadata. They have been “deeply involved” in image and multimedia metadata since then. All <a>IIM</a> properties can be expressed in <a>XMP</a> (the eXtensible Metadata Platform, originally an Adobe spec for embedding metadata in Photoshop, Illustrator, etc. files, which is now an international standard): <A>IPTC</A> Photo Metadata Standard is the metadata vocabulary, <a>XMP</a> is the mechanism for embedding it.
                    </p>
                    <p>
                        They are also deeply involved with Identifiers, recognizing that all “creative work needs an identifier.” “What makes a string an identifier?” In the book industry, identifiers like <a>DOI</a> and <a>ISNI</a> and <a>ISBN</a> are maintained by organizations that formally <em>issue</em> the identifiers. But there’s a big difference between news and books: e.g., a given book publisher might publish 5, 10, or even 1,000 books a week (just the giants), whereas a mid-sized news agency produces 1,000 items <em>per day</em>, and a large news agency can produce 10,000 items <em>per day</em>. Thus they can’t “hand pick an identifier,” this doesn’t scale for news. Instead, they need <em>self-describing</em> identifiers. He pointed out that URIs and URLs have <em>very high relevance</em> for this because they are both an identifier and a carrier of information about what is being identified (unlike the identifiers like <a>DOI</a>, <a>ISNI</a>, and <a>ISBN</a>, which just provide the key to <em>obtain</em> information about what is being identified).
                    </p>
                    <p>
                        Another topic he stressed was the issue of <em>metadata schemas</em>. There are “lots of organizations creating schemas, and many of their properties are quite similar in terms of semantics.” The problem: there are reasonable schemas for different areas of creative work in a given area, but “looking across boundaries it is hard to bring them together.” He pointed out that text, video, etc. all have different schemas. In multimedia, you are often dealing with 5, 6, 7, or 8 different metadata schemas at the same time.
                    </p>
                    <p>
                        The <A>IPTC</A> is trying not to “contribute to metadata proliferation.” Their first rule of thumb: “Is this already defined somewhere?”
                    </p>
                    <p>
                        <em>Rights metadata</em> is very important in this regard. <A>IPTC</A> “will not create its own rights metadata schema.” A book may use content, text, graphics, photos, videos, etc. from news agencies and so they need their metadata to be as consistent as possible. They are working with the W3C Community Groups and have been particularly involved in the development of Open Digital Rights Language (<a>ODRL</a>).
                    </p>
                    <p>
                        They are also a driver of the Linked Content Coalition (LCC), a followup to ACAP (Application Configuration Access Protocol), a group of 40-some organizations (including <a>EDItEUR</a>: both Michael and Graham Bell are directors). They are working on creating a framework for “exchanging rights information across the silos.”
                    </p>
                    <p>
                        Another big topic: <em>metadata values</em>.
                    </p>
                    <p>
                        What’s easy are literal values like dates. Much harder is conceptual information, e.g. “The person in this picture is Mr. So-and-so.” There is a need for a common way to describe entities (people, companies, etc.). Now, with the Semantic Web, it is much more common for each entity to have an identifier.
                    </p>
                    <p>
                        He pointed out that in the context of news, proprietary identifiers get created within given news organizations because of urgency: “they have to do this right away” in order to manage their information. Now there’s a need for a layer for sharing information. One potential solution is a layer on top of Wikipedia, but they are bound to a single language, and Wikipedia links are to a single <em>article</em>. <a href="http://wikidata.org">Wikidata</a> is an important initiative for extracting “entities and topics” and enabling the application of a “generic identifier” that could, e.g., provide a list of “all articles in all languages associated about this entity.”
                    </p>
                    <p>
                        The <A>IPTC</A> has done work on subject categorization (originally “Subject Codes,” now the improved “Media Topics”): over 1,000 terms of content description, which has a “clear focus on news.” <A>IPTC</A> is working on mapping <A>IPTC</A> Media Topics to proprietary vocabularies used in the news industry and then to Wikidata, thus providing a sort of “hub” between all those proprietary schemes and wikidata.
                    </p>
                    <p>
                        <A>IPTC</A> made a formal decision in mid-March that all vocabularies will use <a>SKOS</a> (Simple Knowledge Organization System), which enables “matching” of identifiers at different levels. E.g., “my concept identifier relates to your concept identifier,” but as either an exact match, a subset relationship, or a superset relationship (e.g. “Book Industry” could map as a subset concept to “Economy”).
                    </p>
                    <p>
                        Finally, he talked about <em>formats</em>: how to express a vocabulary, the syntax, etc. He observed that there are “fashions” in formats. “Ten years ago, everything had to be XML; now, XML is old fashioned, everything has to be JSON.” (BTW I hear this “fashion” issue all the time, !)
                    </p>
                    <p>
                        <A>IPTC</A> has a deep involvement in XML, but “with the advent of APIs, XML is too complex, JSON is much simpler.” So now <A>IPTC</A> “needs to reformulate in JSON.” This is an ongoing challenge for standards organizations: “following the fashions of the industry.”
                    </p>
                </section>
                <section>
                    <h2 id="carol-meyer">
                        Carol Meyer, head of Business Development and Marketing for <a>CrossRef</a> and immediate Past President of the Society for Scholarly Publishing.
                    </h2>
                    <p>
                        Carol is in an ideal position to comment on the issue of metadata because <a>CrossRef</a> is a receiver of an enormous amount of metadata from publishers and Carol’s job involves a lot of work directly with the publishers. And as I’ve mentioned in many of these interviews, it is really thanks to <a>CrossRef</a> that metadata is perceived as basically a “solved problem” in the context of scholarly and <a>STM</a> publishing.
                    </p>
                    <p>
                        Carol started by pointing out that one reason for <a>CrossRef</a>’s success and its near universality in the scholarly publishing realm is that it started collecting metadata for a very specific purpose: obtaining just the specific bibliographic metadata required to enable reference linking.
                    </p>
                    <p class="note">
                        This was done initially just for journals, and is now literally a given in journal publishing; a journal article is considered invisible if it doesn’t have a <a>CrossRef</a> <a>DOI</a> and thus isn’t registered in <a>CrossRef</a>. They subsequently added many other publication types—books, conference proceedings, etc.—although one of the things that has made the adoption on the book side slower and less “a given” than on the journal side is that whereas journal content is virtually always online, book content hardly ever is (metadata may be available online but the books themselves are almost always still offline print or ebook products—though scholarly books are some of the most likely of any book content to be online, along with reference).
                    </p>
                    <p>
                        Carol pointed out that this initial simplicity has turned out to be both an advantage and a limitation. The plus side is that it made it quite straightforward for publishers to be able to supply the metadata <a>CrossRef</a> needed to make reference linking work. (Though this is not without its problems; see below.) On the other hand, the very ubiquity of <a>CrossRef</a> caused two other not-necessarily-valid perceptions to form:
                    </p>
                    <ol>
                        <li>That <a>CrossRef</a> could do pretty much anything with metadata. Not so: they can only do what the metadata they get enables them to do. So, for example, publishers would like <a>CrossRef</a> to provide email addresses, but <a>CrossRef</a> has not been collecting email addresses because they weren’t needed for reference linking, and of course now they have a gazillion records that are lacking that information. Extremely non-trivial problem for them to address.
                        </li>
                        <li>That having a <a>CrossRef</a> <a>DOI</a> confers some sort of legitimacy on a journal article. Not so: it just means the article has been properly registered, it says nothing about the quality of the article or the research. But many authors rush to get <a>DOI</a>s purely because without them their article lacks credibility. And sometimes authors and publishers just “make up” <a>DOI</a>s that are not in fact even registered. Of course they don’t “work” in the system, but it’s a headache and friction in the system.
                        </li>
                    </ol>
                    <p>
                        One key observation Carol made is that “there are standards and there are practicalities—and they diverge.” For example, the solution for metadata implementation is probably <a>RDFa</a>, Semantic Web, etc., but this is very hard for the average publisher to do. Giving <a>CrossRef</a> a specific small set of metadata is one thing [and often their prepress or conversion or hosting vendor does that for them anyway] but a true Semantic Web implementation is way beyond the capabilities of all but the largest and most technically savvy publishers.
                    </p>
                    <p>
                        She said that “<a>CrossRef</a> even has a problem interoperating with other registration agencies.” [This even though CrossRef and those other registration agencies are in fact very technically expert.] And this is even more a problem for publishers, especially small or medium size publishers that don’t have the technical expertise, because the tools that are available require programming or at least “a programming mindset,” they don’t “just work.”
                    </p>
                    <p>
                        The big publishers get all this stuff, of course; but the small publishers “are at a real disadvantage” because they still need to work with services that need metadata—e.g. Amazon.
                    </p>
                    <p>
                        There is also a “legacy data problem,” where much of the metadata is “locked up in PDF,” which is a real struggle to deal with.
                    </p>
                    <p>
                        She also pointed out that for books, there is a big frontlist/backlist issue. (Not as much the case for journals, though it depends on the discipline and market demands.) Again, it requires a publisher “of a certain size” to be able to deal with this well.
                    </p>
                    <p>
                        <a>CrossRef</a>’s philosophy has always been “Do the simplest thing that makes it work.” This works for the purpose something was built for, but it gives you legacy issues and doesn’t work for all applications (e.g., the email example mentioned above). Plus you get “nonstandard records” and “compliance issues.”
                    </p>
                    <p>
                        She also pointed out that for any important development, “when there’s a business need, that’s when it happens.”
                    </p>
                    <p>
                        Another big issue in scholarly publishing these days is <em>Data</em>. There is a lot of pressure for the data sets on which research is based to get published, along with the articles and books based on research on that data. We are “hitting a tipping point,” and it is a very complex issue—she characterized it as a “huge unsolved problem.”
                    </p>
                    <p>
                        An important new initiative at <a>CrossRef</a> is FundRef, which is an interesting example of how a de facto standard can come about. There is a need in scholarly publishing to document the funders of research, and there is a great “community of interest” around it. But no standard for funding bodies existed, “so CrossRef made one, and it has become a de facto standard.” It is an example of their “simplest possible solution” strategy. It involves a taxonomy of funder names, and then associates the funders with the papers. There is pressure to make it more complex but right now that’s what was needed, it could be implemented quickly, and it works. She said “at the end of the day it’s all XML tags and identifiers. Everybody using FundRef uses the same set of tags, and it starts becoming a standard.”
                    </p>
                    <p>
                        She pointed out that <a>NISO</a> also very quickly developed an OpenAccess indicator at the article level, involving a “license-ref” URL and a “free-to-read” date [the embargo date] (see <a href="http://www.niso.org/workrooms/oami/"><code>http://www.niso.org/workrooms/oami/</code></a>), so “<a>CrossRef</a> is using that and it works.” Publishers were getting a lot of criticism about not complying with OpenAccess requirements, but it was because there were no systems to deal with it. Something had to be done quickly.
                    </p>
                    <p>
                        She also mentioned that although <a>CrossRef</a> metadata schemas are all initially based on Dublin Core, they are <em>way</em> more complex and rich—they just use Dublin Core as a framework. In fact their <a>CrossMark</a> standard is not based on Dublin Core at all. [<a>CrossMark</a> is the service that enables a link to be embedded in an article that returns, to the user, the information about whether the version they are using is in fact the latest version of an article, and points them to a later version if appropriate—very cool.]
                    </p>
                </section>
                <section>
                    <h3 id="kent-anderson">
                        Kent Anderson, CEO, Journal of Bone and Joint Surgery (JBJS)
                    </h3>
                    <p>
                        Kent is the CEO of a major medical publisher and the current President of the Society of Scholarly Publishing (SSP); previously he pioneered the innovative digital work at the New England Journal of Medicine (NEJM). He is one of the most knowledgeable, articulate, and well-informed people in scholarly publishing (one of those brilliant people who seems never to sleep). So it was particularly telling when his first response was along the lines of “Gee, I haven’t thought about metadata in a while.”
                    </p>
                    <p class="note">
                        If metadata was a big issue—or, more accurately, a big <em>problem</em>—for a medical publisher like this, I guarantee you Kent would be all over it. The fact is, for scholarly publishing, and particularly <a>STM</a> publishing, and even more particularly the M part, medical publishing, in many ways metadata is seen as a solved problem. This is in direct contrast to most other types of publishing, but it was confirmed by others whom I interviewed with this perspective. It’s not that they feel that metadata is perfect, or that it couldn’t be improved; but in contrast to most other types of publishing there is really not much pain there because in the STM sphere metadata has long been a known quantity and pretty much does what they need it to do. (Thanks mainly to <a>CrossRef</a>—see the <a href="#carol-meyer">interview with Carol Meyer</a>.)
                    </p>
                    <p>
                        Indicative of Kent’s forward-thinking nature, he said that the one area where he probably had thought about metadata was in the context of video. That may surprise many folks who think of a scholarly journal publisher as being pretty boring and routine. In fact, publishers like Kent have actually been in the forefront of publishing online and including multimedia and interactivity. He was doing this years ago at NEJM; I’ve been using examples of Kent’s in my talks on the subject for at least the past 5+ years, and have recruited him to speak on the subject on numerous occasions. Most other types of publishers are way behind on this, compared to the leading STM and especially medical publishers.
                    </p>
                    <p>
                        He said that they are now doing “much more multimedia” and observed that there is a lack of good tagging [by which I’m sure he meant subject/semantic tagging] even on things like podcasts. They are also getting into publications that are “beyond journals” and don’t have a good way to manage metadata for them.
                    </p>
                    <p>
                        One metadata issue that Kent astutely picked up on in our conversation was “the portability of metadata.” They have recently updated their XML modeling from NLM to JATS [as are most other STM publishers—these are the XML models that are virtually universally relied upon in the STM realm], and also moved from one hosting service to another. Some of the metadata (particularly semantic metadata) that had been added by the old host was considered proprietary and not transferred, so JBJS had to recreate it for the new hosting context.
                    </p>
                    <p>
                        Another <em>big</em> metadata issue that is just coming to the fore in STM publishing is the need to disclose conflicts of interest. He observed that it is now “very clumsy and not very interoperable.” [Note that the FundRef initiative from <a>CrossRef</a> is an important new vehicle addressing this to some extent.]
                    </p>
                    <p>
                        He also pointed out that they have made investments in things that help bring more visibility into the publication process. They have just acquired <a href="http://pre-val.org">PreScore</a>, a service that provides a metric to assess the level of peer review a given article received, which provides information on pre-publication peer review that “never makes it into the article.” They are also participating in SocialSite, which rates the quality of an article’s reference list.
                    </p>
                </section>
                <section>
                    <h2 id="kevin-hawkins">
                        Kevin Hawkins, Univ. Michigan Libraries and University Press
                    </h2>
                    <p>
                        Until a move to the University of North Texas just after our conversation, Kevin had a very interesting and relevant dual role at the University of Michigan. He has for many years been a key person in the U-M Library’s innovative and extensive Scholarly Publishing Office (now absorbed into Michigan Publishing), one of the pioneers in the trend of academic libraries moving into publishing (online journals, print-on-demand books, the Text Creation Partnership, and more). Kevin is also a true XML expert: he is my go-to guy on anything involving the Text Encoding Initiative (TEI), the XML model dominant in libraries, archives, and humanities scholarship. A couple of years ago, the U-M Library took over responsibility for the University of Michigan Press, and Kevin took major responsibility for production for both print and digital university press monographs.
                    </p>
                    <p>
                        In an interesting confirmation of one of my other calls (also to a scholarly publishing luminary) his first comment was that he was not really aware of any fundamental critical problems regarding metadata. Unlike almost all other segments of publishing, scholarly publishing seems to consider metadata mainly a solved problem (and it largely is).
                    </p>
                    <p>
                        The main problem he pointed to was the inconsistent adoption of metadata schemes throughout the publishing supply chain and the consequential work involved in customizing metadata feeds for each vendor. The Press pays Firebrand (the company run by Fran Toolan, one of my other interviews) to disseminate their metadata, customizing the ONIX feed for various vendors.
                    </p>
                    <p>
                        The U-M LIbrary is a member of <a>CrossRef</a> and deposits DOIs for much of its online content. This requires mapping the metadata they have to the metadata <a>CrossRef</a> requires. While this is annoying, it is not a huge problem because the <a>CrossRef</a> metadata requirements are not extensive. A bigger issue, in Kevin’s view, involves digital workflows. Their publishing platform and its workflows for collections of content was designed for digitized library collections, with infrequent additions to a collection or updates to the content. Revisions are cumulative, with new and revised content not distinguished. So when a new issue of a journal is published they send metadata for all that journal’s issues to <a>CrossRef</a>, relying on <a>CrossRef</a> to screen duplicate records. This of course is an internal U-M issue, but it highlights the fact that metadata is not just about marketing for a publisher like this, it is central to how they manage their workflow.
                    </p>
                    <p>
                        He observed that the Press has not in fact invested much to enhance metadata for discoverability—keywords in HTML, <a>ONIX</a>, <a>BISAC</a>, etc. This needs to be more of a priority. They have done some SEO work to make their online-only content more discoverably in Google search results. But he pointed out that they don’t have “real keywords in microdata,” and they “probably should be doing that.”
                    </p>
                    <p>
                        Another interesting twist from this conversation: promoting discoverability through metadata access actually has no financial return for online-only content, so it gets put off in favor of work for which there is a clearer financial implication. (!!)
                    </p>
                    <p>
                        And finally another important observation that is true of virtually all book publishers but hardly anybody ever brings up: for most books (most of which are not online), there is no HTML to put microdata <em>in</em>!
                    </p>
                    <p>
                        Most of our conversation focused on the Library’s publishing activities, but he did have a few additional comments from the perspective of a library acquiring content and making it available to users:
                    </p>
                    <ul>
                        <li>Because of advances in search and discovery capabilities in library catalogs, vendor databases, and discovery systems, libraries are actually making LESS investment in detailed cataloguing than they used to.
                        </li>
                        <li>Institutional repositories are very widely and heavily used in academia, and most allow self-deposit of content by authors. The author is asked to supply some metadata, and some institutions have a review/validation process.,In his opinion, the thorough crawling of IRs by Google Scholar and other search engines “argues against laborious metadata creation.”
                        </li>
                    </ul>
                </section>
                <section>
                    <h2 id="len-vlahos-julie-morris">
                        Len Vlahos, Executive Director, and Julie Morris, Project Manager, Standards and Best Practices, Book Industry Study Group (BISG)
                    </h2>
                    <p>
                        Len and Julie provide a very interesting perspective on our metadata questions because the <a>BISG</a>—a very broad-based book industry organization comprising a wide range of publishers, retailers, distributors, and service providers across the entire book supply chain (including almost all of the biggest ones)—has a number of committees that specifically focus on metadata issues. They are the US representative to <a>EDItEUR</a> regarding updates to <a>ONIX</a>; they are responsible for <a>BISAC</a>, the US subject vocabulary for the book supply chain; they were a major participant in creating <a>Thema</a>, the new international subject vocabulary; and their Identifiers Committee works closely with the groups responsible for key identifiers like <a>ISBN</a> and <a>ISNI</a> (currently participating in the revision to <a>ISBN</a>). Plus they are involved with rights and manufacturing-oriented metadata (<a>EDI</a>, <a>RFID</a>, etc.).They also do a lot of education as to best practices. This all is precisely the area of Julie’s responsibility, as you’ll see from her title; and Len, as Executive Director, is very actively engaged in all these things. They are an ideal “hub” for insight into the broad book publishing industry, both the creators and the recipients of metadata. I interviewed them jointly. (Julie is a member of the W3C Digital Publishing Interest Group, and BISG is a W3C member, thanks to Len’s realizing how important that is.)
                    </p>
                    <p>
                        Len began by saying that in his view there are two major categories of metadata issues for book publishers: (1) communication, and (2) systems and processes.
                    </p>
                    <p>
                        Regarding communication of metadata, he said that there is a belief from downstream partners [by which he meant the recipients of publishers’ metadata] that publishers are confused by and inconsistent in their use of metadata, whereas upstream, the publishers think the downstream partners are making changes to their metadata, which is unwelcome. [IMO, there is a kernel of truth to both perceptions, although both are exaggerated.]
                    </p>
                    <p>
                        Regarding systems and processes, he pointed out that there are “no clear lines of responsibility for metadata.” “It’s a giant game of telephone that’s gone awry.”
                    </p>
                    <p>
                        Julie specifically addressed issues with <a>ONIX</a>. [In this context, we are talking about ONIX for Books, which is by far the main context in which <a>ONIX</a> is used. There are other versions of <a>ONIX</a>.] Here are some of the key things she pointed out:
                    </p>
                    <ul>
                        <li>There are problems with using metadata to indicate <em>relationships</em> between things, for example a print book from which a digital version was derived.
                        </li>
                        <li>There’s a huge issue in the <em>identifiers</em> sphere [this comes up in almost every group I’m involved in, btw]: the lack of a “Work” identifier as opposed to identifiers for products of that work. [One reason for this is the difference between how publishers view “the work”—they are focused on what they are trying to sell—vs. librarians’ view of “the work” (e.g., “Huckleberry Finn” or “Hamlet,” not one particular publisher’s).]
                        </li>
                        <li><em>versioning</em> is a big issue in digital publications: what’s a new edition vs. a version of an edition.
                        </li>
                        <li>Expressing <em>series</em>, relating products as part of a series or a group of titles that should be tied together.
                        </li>
                    </ul>
                    <p>
                        Another big topic discussed by both Len and Julie was the volatile nature of the situation, because of changes in the industry and the types of products being produced.
                    </p>
                    <p>
                        How does “product metadata” relate to metadata embedded in a digital product? These are handled quite separately in most publishing organizations, and there is a lack of awareness and communication between departments (digital, production, marketing, etc.). Fundamentally there is a lack of clarity and consistency regarding “what are we saying about this thing?”
                    </p>
                    <p>
                        Plus, there is a need for metadata to describe changing products, which results in “mushrooming metadata.” Publishers like Pearson wind up thinking of themselves increasingly as technology companies.
                    </p>
                    <p>
                        Len pointed out another key problem: the book industry is no longer “all one thing” as it was in the past. There are so many bidirectional relationships between types of publishers and the entities they deal with (trade⇿retailer, trade⇿library, educational publisher⇿school system, etc.), and there is “no overriding standard that accommodates all of these.”
                    </p>
                    <p>
                        Len also pointed out that there have been discussions with <a>GS1</a> [the organization responsible for global standards for the supply chain, best known for barcodes] and the book industry metadata was actually “in relatively good shape” compared to that in other sectors like apparel, beverages, etc. The book industry is “more similar to music: so many different SKUs.”
                    </p>
                    <p>
                        He also mentioned [surprisingly to me] that some have begun to question the long term value of <a>ONIX</a> to the industry. He pointed out that lots of work has been done on the library side regarding linked data. He suggested that there was an evolution toward looking at metadata not at a “record” level [e.g., an <a>ONIX</a> record] but at a more distributed level.
                    </p>
                    <p>
                        Julie pointed out the inherent conflict—or at least the co-existence, and at present not all that good alignment—between <a>ONIX</a>, <a>MARC</a>, and <a>schema.org</a>.
                    </p>
                    <p>
                        Len pointed out that eventually the <em>data</em> about the book [not sure if he meant just metadata or the content itself] will reside in the cloud.
                    </p>
                    <p>
                        He gave the example of author vs. contributor metadata. There is important granular information connecting contacts, contracts, rights, etc. to authors and contributors and their products, and this is not best accomplished by “boxing it into either an <a>ONIX</a> or <a>MARC</a> record” [his implication being, if I can speculate, that this both makes that metadata inaccessible to updating and unnecessarily duplicates information that is really the same in a bunch of those different “boxes”]. A more “networked” approach [my word, not his] would lend itself to greater conformity, which would be all to the good.
                    </p>
                    <p>
                        Finally, he also pointed out that there is an inherent conflict between the needs of archivists/cataloguers and marketers/publishers [alluded to above in discussing the library vs. publisher pov on things like what “a work” is].
                    </p>
                </section>
            </section>
            <section>
                <h3 id="vertical">
                    “Vertical” Interviews (Madi Solomon)
                </h3>
                <p>
                    <em>A small controlled survey of Pearson’s thoughts on metadata</em>
                </p>
                <section>
                    <h4>
                        Executive Summary
                    </h4>
                    <p>
                        Metadata, data about data, has been a conversation piece in the publishing industry for years now but proving its usefulness to the businesses has remained elusive. While most people can speak of the importance of metadata and how it has been successfully monetized by the likes of Facebook, Amazon and Google, in local practices it is still perceived as a labour intensive manual effort with few redeemable benefits. Even with past efforts of applying minimal metadata, our assets remained un-discoverable, rights information are locked and difficult to obtain, and federated search provides scant returns.
                    </p>
                    <p>
                        This persistent reality has remained despite several enterprise deployments of different asset management systems over the past ten years. The systems were not at fault, but our process was. Many large publishing organisations have never had a global strategy. Investments in digital asset management and rights systems tended to concentrate in the US and UK leaving significant international regional offices such as Brazil, Australia, and Mexico, completely isolated, without tools or access.
                    </p>
                    <p>
                        This approach to asset and content management must change if we are to unite around a single suite of technologies that streamline global access, embed efficacy measures, and enable digital distributions across all devices and platforms.
                    </p>
                    <p>
                        In preparation for a Pearson Metadata &amp; Taxonomy Roadmap, Madi Solomon conducted 12 anonymous interviews with education publishing representatives across many different Lines of Business (mostly from US, UK, and Canada). This report synthesizes the results of the survey.
                    </p>
                    <section>
                        <h4>
                            Sponsorship
                        </h4>
                        <p>
                            These interviews were commissioned by the Semantic Platforms &amp; Metadata team of the Core Platforms and Enterprise Architecture (Pearson Technology) and the W3C Digital Publishing Interest Group, Metadata Taskforce, in the quest to answer the question “What are the metadata pain points for publishers as they evolve to digital distribution?”
                        </p>
                    </section>
                    <section>
                        <h3>
                            Goal
                        </h3>
                        <p>
                            The results of this survey inform two goals:
                        </p>
                        <ol>
                            <li>Results have been combined with another set of interviews that were conducted by the Co-Chairs (Madi Solomon - Pearson, Bill Kasdorff - Apex CoVantage) of the <a href="https://www.w3.org/dpub/IG/wiki/Task_Forces/Metadata">W3C Digital Publishing Interest Group, Metadata Taskforce</a>, to provide a broad view of publishing challenges around metadata.
                            </li>
                            <li>Inform the Metadata &amp; Taxonomy Roadmap to be created by Ian Piper, Chief Enterprise Taxonomist and Madi Solomon, Semantic Platforms, to ensure business relevance across the enterprise.
                            </li>
                        </ol>
                    </section>
                    <section>
                        <h3>
                            Methodology
                        </h3>
                        <p>
                            One-to-one half-hour interviews were conducted over a four week period in March-April 2014. The interviews proceeded as anonymous, casual and candid conversations on experiences or observations around metadata.
                        </p>
                    </section>
                </section>
                <section>
                    <h3>
                        Results
                    </h3>
                    <p>
                        When the results of the W3C Metadata Task Force were combined with those from this report, differences in metadata expectations between Trade and Education publishing surfaced. The majority of those interviewed by Bill Kasdorf were trade publishers while Pearson, still in its transformative state to digital, were more focused on modularising content. Some of these differences are exampled in the table below.
                    </p>
                    <table class="zebra">
                        <thead>
                            <tr>
                                <th valign="top">
                                    Trade
                                </th>
                                <th valign="top">
                                    Education Publishers
                                </th>
                            </tr>
                        </thead>
                        <tr>
                            <td valign="top">
                                Trade publishers stated that metadata complexity, mostly with <a>ONIX</a>, was a challenge to their business
                            </td>
                            <td valign="top">
                                Not a single interviewee mentioned <a>ONIX</a> or any other industry standard.
                            </td>
                        </tr>
                        <tr>
                            <td valign="top">
                                Trade publishers lamented the many metadata vocabularies (<a>ONIX</a>, <a>BISAC</a>, <a>PRISM</a>, etc.) and the difficulty in keeping up-to-date on all of them.
                            </td>
                            <td valign="top">
                                One interviewee mentioned multiple metadata standards and vocabularies as an issue
                            </td>
                        </tr>
                        <tr>
                            <td valign="top">
                                “Few books are online anyway,” was a general response from Trade. Other than <a>STEM</a> journals and articles, traditional publishers considered books as whole products and rarely modularised or componentized content. Metadata was relegated to Title and Author and not much more.
                            </td>
                            <td valign="top">
                                This did not apply to any representative as modular education content, personalised content, learner outcomes and efficacy measures were all based on data. The ability for personalisation (by a student, teacher, or institution) was a top priority for education publishers.
                            </td>
                        </tr>
                        <tr>
                            <td valign="top">
                                <a>ONIX</a> vs. Subject Metadata was a common debate in Trade. <a>ONIX</a> was originally created for the supply chain (retailers, etc), primarily for physical books and has since been updated (<a>ONIX</a> 3.0) for eBooks. There was general resistance to <a>ONIX</a> 3.0 because publishers believed 2.1 was fine and what the supply chain demanded. Subject or descriptive metadata on the intellectual content of a book was not easily supported or embedded in <a>ONIX</a>.
                            </td>
                            <td valign="top">
                                Subject metadata was a key entry point for educational content and information such as Learning Outcomes/Objectives and Learner levels were considered essential.
                            </td>
                        </tr>
                        <tr>
                            <td valign="top">
                                Trade publishers recognized the need for Keywords for books, chapters and component discovery, but were not necessarily interested in a controlled vocabulary. Trade publishers were only just beginning to realise that search engines did not use Library Catalogues to find book titles.
                            </td>
                            <td valign="top">
                                Many interviewees stated that many controlled vocabularies were required to optimise discovery and to re-purpose existing content.
                            </td>
                        </tr>
                    </table>
                </section>
                <section>
                    <h4>
                        Education Publisher’s Results
                    </h4>
                    <p>
                        Percentages refer to the number of respondents who mentioned these topics as top metadata priorities for their business.
                    </p>
                    <section>
                        <h5>
                            Governance—100%
                        </h5>
                        <p>
                            Every respondent cited the lack of metadata governance and authority as a major issue in their daily interactions with metadata. There wasn’t an authority to help dictate metadata requirements or to help embed or impose standards across the workflow. Instead, the “right to refuse” remained steeped in the traditional business culture where editorial had the authority to reject anything. The right to reject was also scattered across the workflow.
                        </p>
                        <blockquote>
                            <p>
                                Quotes:
                            </p>
                            <p>
                                “We’re nowhere on this. Every publisher right now adds and amends metadata at will so there is no cohesive approach.”
                            </p>
                            <p>
                                “Governance is critical in ensuring that customers have a good experience with our platforms.”
                            </p>
                            <p>
                                “We need to standardise on format for dates, for example, and make everyone use the ISO standards. This would have a very positive impact overall.”
                            </p>
                            <p>
                                “Governance is a pain point. In the past, governance boards were formed by non-metadata experts. We should be adopting industry standards and <em>impact</em> should not be a decision choice.”
                            </p>
                            <p>
                                “There are no mechanisms, governance, or levers that apply metadata, so the businesses just remain frustrated.”
                            </p>
                        </blockquote>
                    </section>
                    <section>
                        <h5>
                            Rights—60%
                        </h5>
                        <p>
                            A majority of respondents stated that Rights Information was one of the most important metadata issues facing publishers at large. Without trusting Rights data, the businesses would rather err on caution and re-create or re-commission content before re-using or re-purposing existing content. The lack of Rights information on content and assets was costing organisations a small fortune in duplication and litigation. A means for querying rights information from source content/titles to the many derivatives was a priority need. While Rights Information was available in internal rights systems, the difficulty and the long wait in getting requested information was a source of frustration to many.
                        </p>
                        <blockquote>
                            <p>
                                QUOTES:
                            </p>
                            <p>
                                “Our legacy systems are intractable and need to be abandoned.”
                            </p>
                            <p>
                                “We are six months behind in rights clearances for digital delivery and this is a major bottleneck.”
                            </p>
                            <p>
                                “There really shouldn’t be a separation anymore between the technologies that handles rights information. We need a new global rights strategy.”
                            </p>
                            <p>
                                “U.S. Rights can be cleared in our rights system … but there is no consideration for data governance.”
                            </p>
                            <p>
                                “We’re getting better at this by getting getter rights interactions between people. Now we need to get our systems to interact.”
                            </p>
                        </blockquote>
                    </section>
                    <section>
                        <h5>
                            Flow—60%
                        </h5>
                        <p>
                            60% of the respondents think that the flow of metadata was seriously compromised. There were many opportunities for metadata to be inherited, but no measures or mechanisms were created to capture them in the content lifecycle. Metadata was an afterthought in most workflows. Automation existed only in scraping exercises long after the content or assets were created, leaving some guesswork to vital information such as rights or identifying source originals to the distributors. The onus of applying metadata, data clean-up, and format conversions were left to the receiving platforms. This could be ameliorated with a more holistic coordinated view.
                        </p>
                        <blockquote>
                            <p>
                                QUOTES:
                            </p>
                            <p>
                                “Metadata doesn’t flow upstream! We should fix this whatever we do.”
                            </p>
                            <p>
                                “A hybrid approach would be ideal, where humans populate some fields with controlled vocabularies and the rest auto-populated or scraped.”
                            </p>
                            <p>
                                “There is a general confusion between metadata, the process and flows of metadata, and what and when it’s captured.”
                            </p>
                            <p>
                                “The current workflow is more bothered than helped by metadata.”
                            </p>
                            <p>
                                “We end up being in the conversion business rather than the content delivery business. There are glaring inefficiencies, if not outright broken components, in our workflow and our ways of working.”
                            </p>
                            <p>
                                “Metadata is currently a myth, it simply does not exist. So what is its value? Metadata’s value can only be measured by its application so we’re stuck in a Catch 22: there is no metadata so there is no value so there is no use case.”
                            </p>
                        </blockquote>
                    </section>
                    <section>
                        <h5>
                            Lack of Skills—50%
                        </h5>
                        <p>
                            In our current changing culture, modular content demands more disciplined data care, but the businesses have not caught up to this. In general, the businesses don’t have the knowledge or skills to design a metadata growth model and by default, expect it to be done by someone else. This option, while valid, has not been formalised in any way and consequently, no such entity exists (other than vendors). There were many recommendations for a centralised service to help with this (See Centralised Service—40%). Overall, the businesses requested more help in defining the new rules of engagement.
                        </p>
                        <blockquote>
                            <p>
                                QUOTES:
                            </p>
                            <p>
                                “We are not data specialists, and this is all about data.“
                            </p>
                            <p>
                                “There are missing links between the workflows and information.”
                            </p>
                            <p>
                                “We need more education in order to unlock the potential of metadata.”
                            </p>
                            <p>
                                “It’s a chicken and egg thing: how to innovate and still support key business functionality?”
                            </p>
                        </blockquote>
                    </section>
                    <section>
                        <h5>
                            Lack of Authority—50%
                        </h5>
                        <p>
                            Half of the respondents collectively wanted more authority around metadata. The businesses wanted to know who “owned” metadata. Rather than leaving it up to the businesses, or the editorial process, they requested a stronger authority that could better support and enforce the required standards and could extend this authority to influence technology measures that ensured compliance. This related directly to the Governance issues around metadata, which further substantiated the need for a centralised entity to fully manage, monitor, and govern metadata standards.
                        </p>
                        <blockquote>
                            <p>
                                QUOTES:
                            </p>
                            <p>
                                “Consensus takes too much time.”
                            </p>
                            <p>
                                “There are no governing principles so we are capturing metadata but without a good story. By story, I mean we don’t know the worth of the effort.”
                            </p>
                            <p>
                                “There would be more acceptance if we had a stronger top-down mandate.”
                            </p>
                            <p>
                                “We need a central function of metadata, taxonomy, and vocabularies with an authority that manages it.”
                            </p>
                            <p>
                                “We need a balance between enforced standards through tools and extending standards for specific business needs.”
                            </p>
                            <p>
                                “Metadata was the responsibility of the engineers and developers of the system. The role of metadata should be shifted to a specialist.”
                            </p>
                        </blockquote>
                    </section>
                    <section>
                        <h5>
                            Standards—50%
                        </h5>
                        <p>
                            Metadata standards were cited as something that should be identified, imposed, and managed. This included all forms of metadata from technical, structural, descriptive, search optimisation, educational standards and curriculum, all the way to online delivery standards. The use of standards was the solution for keeping content fluid enough to be shared and distributed across multiple platforms and devices. These were not solely for content, however, but also applied to consumer and learner data. These standards were recognised as essential in realising goals for personalisation and recommendations.
                        </p>
                        <blockquote>
                            <p>
                                QUOTES:
                            </p>
                            <p>
                                “We need to implement minimum metadata standards and vocabularies.”
                            </p>
                            <p>
                                “The problem is that terms are dictated by owners so there are conflicts between systems because they are not standardised.”
                            </p>
                            <p>
                                “Rather than working groups coming up with standards, we should use educational standards already in existence.”
                            </p>
                            <p>
                                “If I had a magic wand, I’d build collaborations between product and services. Product has been isolated and really, 75% of metadata should be relevant across all platforms. I’d normalize metadata.”
                            </p>
                            <p>
                                “No one is aware of what standards we’re supposed to use.”
                            </p>
                            <p>
                                “We need to make standards adoptable. Right now it’s too difficult to get people to change.”
                            </p>
                            <p>
                                “Fundamental change in our process is necessary.”
                            </p>
                        </blockquote>
                    </section>
                    <section>
                        <h5>
                            Other topics
                        </h5>
                        <p>
                            The following topics were referenced by 40% of the respondents:
                        </p>
                        <ul>
                            <li>
                                <em>Inconsistency</em>
                                <p>
                                    Multiple interpretations of standards, multiple definitions of terms and regionalisms have complicated the field.
                                </p>
                            </li>
                            <li>
                                <em>Lack of Incentives</em>
                                <p>
                                    Organisations often lack incentives for the businesses to work towards a “greater good.” Metadata was consistently sacrificed to aggressive delivery dates when the effort of applying it offered little or no benefits outside of the program.
                                </p>
                            </li>
                            <li>
                                <em>Need for Learning Objectives</em>
                                <p>
                                    Learning Objectives/Outcomes were referenced as key metadata for ensuring compliance with specific curricula. This was considered as important as descriptive metadata.
                                </p>
                            </li>
                            <li>
                                <em>Need for a centralised authority or entity</em>
                                <p>
                                    Closely related to Governance and Authority, a little less than half the respondents wanted a centralised group to solve metadata problems and offer services to accelerate access to and ingestion of metadata, vocabularies, learning objectives, and format conversions.
                                </p>
                            </li>
                            <li>
                                <em>More education and guidance</em>
                                <p>
                                    Many of the interviewees wanted more educational resources around metadata. Recognising that publishing was fairly new to content/asset/product management based on metadata, they requested more guidelines on the subject.
                                </p>
                            </li>
                        </ul>
                    </section>
                </section>
                <section>
                    <h3>
                        Conclusion
                    </h3>
                    <p>
                        Metadata touches many parts of the digital supply chain, yet a comprehensive approach to its application and its value has been poorly executed. Reasons for this is includes a long list of exhausted efforts with good intentions. The businesses, however, may now be ready to embrace changes to their traditional approaches to content creation and are particularly open to the prospect of data-driven workflows that extend to efficacy metrics and personalisation of learning objects.
                    </p>
                </section>
            </section>
        </section>
        <section class="appendix">
            <h2>Acronyms and Terms Used in the Inteviews</h2>
            <ul>
                <li><dfn>BISAC</dfn>: Book Industry Standards and Communications, <a href="https://www.bisg.org/complete-bisac-subject-headings-2013-edition"><code>https://www.bisg.org/complete-bisac-subject-headings-2013-edition</code></a></li>
                <li><dfn>BISG</dfn>: Book Industry Study Group, <a href="http://www.bisg.org"><code>http://www.bisg.org</code></a></li>
                <li><dfn>CrossMark</dfn>: identification service from <a>CrossRef</a>, <a href="http://www.crossref.org/crossmark/"><code>http://www.crossref.org/crossmark/</code></a></li>
                <li><dfn>CrossRef</dfn>: association of scholarly publishers that develops shared infrastructure to support more effective scholarly communications, <a href="http://www.crossref.org"><code>http://www.crossref.org</code></a></li>
                <li><dfn>EDI</dfn>: Electronic Data Interchange <a href=""></a></li>
                <li><dfn>DOI</dfn>: Digital Object Identifier, <a href="http://www.doi.org"><code>http://www.doi.org</code></a></li>
                <li><dfn>EDItEUR</dfn>: international group coordinating development of the standards infrastructure for electronic commerce in the book, e-book and serials sectors, <a href="http://www.editeur.org"><code>http://www.editeur.org</code></a></li>
                <li><dfn>Fundref</dfn>: provides a standard way to report funding sources for published scholarly research, <a href="http://www.crossref.org/fundref/"><code>http://www.crossref.org/fundref/</code></a></li>
                <li><dfn>GS1</dfn>: international not-for-profit association that develops and maintains standards for supply and demand chains across multiple sectors, <a href="http://www.gs1.org"><code>http://www.gs1.org</code></a></li>
                <li><dfn>IPTC</dfn>: International Press and Telecommunications Council, <a href="http://www.iptc.org"><code>http://www.iptc.org</code></a></li>
                <li><dfn>IPTC 7901</dfn>: text transmission format, <a href="http://www.iptc.org/site/News_Exchange_Formats/IPTC_7901/"><code>http://www.iptc.org/site/News_Exchange_Formats/IPTC_7901/</code></a> </li>
                <li><dfn>IIM</dfn>: Information Interchange Model, <a href="http://www.iptc.org/cms/site/index.html?channel=CH0108"><code>http://www.iptc.org/cms/site/index.html?channel=CH0108</code></a></li>
                <li><dfn>ISBN</dfn>: International Standard Book Number, <a href="http://www.isbn-international.org/"><code>http://www.isbn-international.org/</code></a></li>
                <li><dfn>ISNI</dfn>: International Standard Name Identifier, <a href="http://www.isni.org"><code>http://www.isni.org</code></a></li>
                <li><dfn>JATS</dfn>: Journal Article Tag Suite, <a href="http://jats.nlm.nih.gov"><code>http://jats.nlm.nih.gov</code></a></li>
                <li><dfn>MARC</dfn> (usually referring to MARC 21): MAchine-Readable Cataloging, <a href="http://www.loc.gov/marc/bibliographic/ecbdhome.html"><code>http://www.loc.gov/marc/bibliographic/ecbdhome.html</code></a></li>
                <li><dfn>NewsML</dfn>: an XML standard designed to provide a media-independent, structural framework for multi-media news, <a href="http://www.newsml.org/"><code>http://www.newsml.org/</code></a></li>
                <li><dfn>NISO</dfn>: National Information Standards Organization, <a href="http://www.niso.org/"><code>http://www.niso.org/</code></a></li>
                <li><dfn>ODRL</dfn>: Open Digital Rights Language, <a href="http://www.w3.org/community/odrl/two/model/"><code>http://www.w3.org/community/odrl/two/model/</code></a> </li>
                <li><dfn>ONIX</dfn> (usually referring to ONIX for Books in this document): international standard for representing and communicating book industry product information in electronic form, <a href="http://www.editeur.org/11/Books/"><code>http://www.editeur.org/11/Books/</code></a></li>
                <li><dfn>ORCID</dfn>: Open Researcher and Contributor ID, <a href="http://orcid.org/"><code>http://orcid.org/</code></a></li>
                <li><dfn>PRISM</dfn>: Publishing Requirements for Industry Standard Metadata, <a href="http://www.idealliance.org/specifications/prism-metadata-initiative/prism"><code>http://www.idealliance.org/specifications/prism-metadata-initiative/prism</code></a> </li>
                <li><dfn>RFID</dfn>: Radio-frequency identification</li>
                <li><dfn>RDFa</dfn>: specification to express RDF data in HTML, <a href="http://rdfa.info"><code>http://rdfa.info</code></a></li>
                <li><dfn>RightsML</dfn>: IPTC's Rights Expression Language for the media industry, <a href="http://www.iptc.org/site/RightsML/"><code>http://www.iptc.org/site/RightsML/</code></a> </li>
                <li><dfn>rNews</dfn>: an IPTC vocabulary for using semantic markup to annotate news-specific metadata in HTML documents, <a href="http://dev.iptc.org/rNews"><code>http://dev.iptc.org/rNews</code></a></li>
                <li><dfn>schema.org</dfn>: initiative to create and support a common set of schemas for structured data markup on web pages, <a href="http://schema.org"><code>http://schema.org</code></a></li>
                <li><dfn>SKOS</dfn>: Simple Knowledge Organization System, <a href="http://www.w3.org/2004/02/skos/"><code>http://www.w3.org/2004/02/skos/</code></a></li>
                <li><dfn>STEM</dfn>: Science, Technology, Education, and Medical (publishing)</li>
                <li><dfn>STM</dfn>: Science, Technology, and Medical (publishing)</li>
                <li><dfn>Thema</dfn>: subject category scheme for a global book trade, <a href="http://www.editeur.org/151/Thema/"><code>http://www.editeur.org/151/Thema/</code></a></li>
                <li><dfn>UPC</dfn>: Universal Product Code</li>
                <li><dfn>XMP</dfn>: Extensible Metadata Platform, <a href="http://partners.adobe.com/public/developer/en/xmp/sdk/XMPspecification.pdf"><code>http://partners.adobe.com/public/developer/en/xmp/sdk/XMPspecification.pdf</code></a> </li>
        </section>        
        <section class="appendix">
            <h2>
                Footnotes
            </h2>
            <div id="ftn1">
                <p>
                    <a class="footnote_sign" href="#_ftnref1" title="" id="_ftn1">1</a> The one exception was Mr. Kasdorf’s <a href="len-vlahos-julie-morris">interview with Len Vlahos, Executive Director, and Julie Morris, Director of Standards and Best Practices</a>, both with BISG, the Book Industry Study Group.
                </p>
            </div>
            <div id="ftn2">
                <p>
                    <a class="footnote_sign" href="#_ftnref2" title="" id="_ftn2">2</a> JATS, the Journal Article Tag Suite, and BITS, the Book Interchange Tag Suite-which share a common markup model below the article and chapter level and which have very rich metadata models and mechanisms-are the current versions of what were previously known as the “NLM DTDs,” the markup and metadata model on which virtually all publications, platforms, and services in the area of scholarly publishing are based. This is unique to scholarly publishing: in no other sector is there such universal consensus on a single markup and metadata model.
                </p>
            </div>
        </section>
    </body>
</html>
